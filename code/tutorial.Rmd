---
title: "Introduction to Spark in R (using `sparklyr`)"
author: "Justyna Jankowiak"
date: "07-08.10.2017"
output: html_document
---

# Workflow

The typical workflow has three steps:

1. Connect,
2. Do some work,
3. Disconnect.

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
library(sparklyr)

# connect
conn <- spark_connect(master = "local")
# It's possible that you have to set JAVA_HOME and SPARK_HOME variables 
# Sys.setenv(JAVA_HOME = "/usr/lib/jvm/java-8-openjdk-amd64")
# Sys.setenv(SPARK_HOME = "/home/justyna/spark/spark-2.2.0-bin-hadoop2.7")

# do some work
spark_connection_is_open(conn)

# disconnect
spark_disconnect(conn)
```

`spark_connect()` takes argument `master` which is url to Spark cluster to connect to. We use `"local"` to connect to local instance of Spark install via `spark_install()`.

Now we can check if the connection is open using `spark_connection_is_open()` with the name of created connection as its argument.

After our work is done we can close the connection with `spark_disconnect(conn)`.

# Getting data

There are to ways to get data into Spark:

1. reading data directly from a file (e.g. csv)
2. copying `data.frame` from R

Loading data is a slow process - when working with big data we want to avoid copying data from one location to another.

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# For this chunk and all following once we assume that the sparklyr library is loaded 
# and connection to Spark is established

# read data from csv 
spark_read_csv(conn, "wine", "data/wine.csv")

# copy data.frame from R
library(MASS) # for dataset Boston
library(dplyr) # for copy_to() function
copy_to(conn, Boston, "house_prices")

# check all available tables stored in Spark
src_tbls(conn)
```
To read data from file we use functions `spark_read_*()`. They all take three obligatory arguments - a Spark connection, name to assign and path to the file.

We can also copy data directly from R with `dplyr`'s `copy_to()` function which takes connection and local data frame as argument. We can also specify name for new remote table but it is not necessary.

After loading datasets we can list all tables stored in Spark with `src_tbls()` function.

# Exploring data

In Spark the data is stored in the variable called `DataFrame` which is more or less the same as R's `data.frame`. If we want to explore the data stored in Spark we have to locally create object which has connection to remote data. 

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# create reference to Spark table without loading it into memory
house_prices_tbl <- tbl(conn, "house_prices")

# is tibble?
is.tbl(house_prices_tbl)

# print data (default number of rows = 10)
print(house_prices_tbl)

# print first 5 rows with the width of 25 characters
print(house_prices_tbl, width = 25)

# examine the stucture with str() function
str(house_prices_tbl)

# examine the structure with glimpse() function
glimpse(house_prices_tbl)

# the size of data stored in R
object.size(Boston)

# the size of data stored in R as a tibble
object.size(house_prices_tbl)
```

Calling `tbl()` with a Spark connection and the name of table stored in Spark will create object of type `tibble`. The tibble object doesn't keep a copy of the data itself only reference to it. The `print()` method uses Spark connection stored in tibble object, copies a piece of the data back to R and displays it. We can manipulate how many rows and columns we want to display by changing arguments `n` and `width`. We use `glimpse()` function to examine the structure of the data, because `str()` doesn't know how to retrieve tha data from remote source.

We can see by analyzing results of `object.size()` that `tibble` object is smaller than the same structure stored in R, because it stores only connection to to the romote data, not the data itself.

# Manipulating data using `dplyr` syntax

There are five main actions you can do with data using `dplyr`:

- select columns
- filter rows
- arrange rows
- change or add columns
- calculate summary statistics

All operations are performed on tibble objects.

## Select columns

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# select columns
# crim - per capita crime rate by town
# age - proportion of owner-occupied units built prior to 1940
# medv - median value of owner-occupied homes in 1000$
house_prices_tbl %>% 
  select(crim, age, medv)

# note that square bracket indexing is not supported 
house_prices_tbl[, c("crim", "age", "medv")]

# choose all but crim, age, medv
house_prices_tbl %>% 
  select(-crim, -age, -medv)

# select columns between nox and tax
house_prices_tbl %>% 
  select(nox:tax)

# select all columns starting from "r"
house_prices_tbl %>% 
  select(starts_with("r"))

# select all columns containing "rat" 
house_prices_tbl %>% 
  select(contains("rat"))

# select all columns matching regular expression (ends with "s")
house_prices_tbl %>% 
  select(matches("s$"))

# select all columns matching regular expression (two "t"s with some letter(s) between)
house_prices_tbl %>% 
  select(matches("t.+t"))
```

Selecting columns id done by function `select()` with unquoted names of the columns you want to keep. You can also select columns by excluding some columns, with `-` before the name.

Sometimes is easier not to call names of ther columns directly. When we want to select columns which are next to each other we can use them using syntax `name_of_the_column_to_start:name_of_the_column_to_end`. We can also specify the names of columns we want to keep by calling `starts_with()` and `ends_with()` methods which will keep only the columns with the names starting (of ending) with string specified as an argument. Method `contains()` with some string as an argument will keep columns which contain in the name this string. Finally, we can also use regular expressions with the method `matches()` to select specific columns.

## Filter rows
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# filter rows
# rm - average number of rooms per dwelling > 8
house_prices_tbl %>% 
  filter(rm > 8)

# ptratio - pupil-teacher ratio < 15
# crim - per capita crime rate by town < 0.005
house_prices_tbl %>% 
  filter(ptratio < 15, crim < 0.02)
```

We can filter the rows by using function `filter()` with logical conditions on the columns as the arguments. The number of operations is limited but we can use comparison, mathematical and logical operators and many mathematical functions such as `log()` or `abs()`.

## Arrange rows
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# arrange by age in ascending order
house_prices_tbl %>% 
  arrange(age)

# arrange by crim in ascending order and by rm in descending order
house_prices_tbl %>% 
  arrange(crim, desc(rm))
```

To arrange rows in specific order we use `arrange()` function. Default sorting is in ascending order but we can use `desc()` method on the column to sort it in descending order.

## Change or add columns
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# change tax column by multiplying it by 10 000
house_prices_tbl %>% 
  mutate(tax = tax * 10000)

# create new column is_cheap which is transformation of medv column
house_prices_tbl %>% 
  mutate(is_cheap = ifelse(medv < 25, "cheap", "expensive"))

# create new column crim_medv_ratio which is crim to medv proportion
house_prices_tbl %>% 
  mutate(crim_medv_ratio = crim / medv)
```

Using `mutate()` function allows us to transform existing column or create a new one. The function takes arguments in the form `name_of_the_column = value_of_the_column`. When updating or adding the column you can operate on more than one column to count its value. 

## Calculate summary statistics
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# calculate mean medv
house_prices_tbl %>% 
  summarise(mean_medv = mean(medv))

# calculate min dis and sd of tax
house_prices_tbl %>% 
  summarise(min_dis = min(dis), sd_tax = sd(tax))
```
`summarise()` function is used to calculate summary of a column (e.g `min()`, `max()` or `mean()` value). The object returned by this function is also a tibble, even if it's only one single value.


