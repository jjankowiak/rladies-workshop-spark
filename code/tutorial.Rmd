---
title: "Introduction to Spark in R (using `sparklyr`)"
author: "Justyna Jankowiak"
date: "07-08.10.2017"
output: html_document
---

# Workflow

The typical workflow has three steps:

1. Connect,
2. Do some work,
3. Disconnect.

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
library(sparklyr)

# connect
conn <- spark_connect(master = "local")
# It's possible that you have to set JAVA_HOME and SPARK_HOME variables 
# Sys.setenv(JAVA_HOME = "/usr/lib/jvm/java-8-openjdk-amd64")
# Sys.setenv(SPARK_HOME = "/home/justyna/spark/spark-2.2.0-bin-hadoop2.7")

# do some work
spark_connection_is_open(conn)

# disconnect
spark_disconnect(conn)
```

`spark_connect()` takes argument `master` which is url to Spark cluster to connect to. We use `"local"` to connect to local instance of Spark install via `spark_install()`.

Now we can check if the connection is open using `spark_connection_is_open()` with the name of created connection as its argument.

After our work is done we can close the connection with `spark_disconnect(conn)`.

# Getting data

There are to ways to get data into Spark:

1. reading data directly from a file (e.g. csv)
2. copying `data.frame` from R

Loading data is a slow process - when working with big data we want to avoid copying data from one location to another.

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# For this chunk and all following once we assume that the sparklyr library is loaded 
# and connection to Spark is established

# read data from csv 
spark_read_csv(conn, "wine", "data/wine.csv")

# copy data.frame from R
library(MASS) # for dataset Boston
library(dplyr) # for copy_to() function
copy_to(conn, Boston, "house_prices")

# check all available tables stored in Spark
src_tbls(conn)
```
To read data from file we use functions `spark_read_*()`. They all take three obligatory arguments - a Spark connection, name to assign and path to the file.

We can also copy data directly from R with `dplyr`'s `copy_to()` function which takes connection and local data frame as argument. We can also specify name for new remote table but it is not necessary.

After loading datasets we can list all tables stored in Spark with `src_tbls()` function.

# Exploring data

In Spark the data is stored in the variable called `DataFrame` which is more or less the same as R's `data.frame`. If we want to explore the data stored in Spark we have to locally create object which has connection to remote data. 

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# create reference to Spark table without loading it into memory
house_prices_tbl <- tbl(conn, "house_prices")

# is tibble?
is.tbl(house_prices_tbl)

# print data (default number of rows = 10)
print(house_prices_tbl)

# print first 5 rows with the width of 25 characters
print(house_prices_tbl, width = 25)

# examine the stucture with str() function
str(house_prices_tbl)

# examine the structure with glimpse() function
glimpse(house_prices_tbl)

# the size of data stored in R
object.size(Boston)

# the size of data stored in R as a tibble
object.size(house_prices_tbl)
```

Calling `tbl()` with a Spark connection and the name of table stored in Spark will create object of type `tibble`. The tibble object doesn't keep a copy of the data itself only reference to it. The `print()` method uses Spark connection stored in tibble object, copies a piece of the data back to R and displays it. We can manipulate how many rows and columns we want to display by changing arguments `n` and `width`. We use `glimpse()` function to examine the structure of the data, because `str()` doesn't know how to retrieve tha data from remote source.

We can see by analyzing results of `object.size()` that `tibble` object is smaller than the same structure stored in R, because it stores only connection to to the romote data, not the data itself.

# Manipulating data using `dplyr` syntax

There are five main actions you can do with data using `dplyr`:

- select columns
- filter rows
- arrange rows
- change or add columns
- calculate summary statistics

All operations are performed on tibble objects.

## Select columns

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# select columns
# crim - per capita crime rate by town
# age - proportion of owner-occupied units built prior to 1940
# medv - median value of owner-occupied homes in 1000$
house_prices_tbl %>% 
  select(crim, age, medv)

# note that square bracket indexing is not supported 
house_prices_tbl[, c("crim", "age", "medv")]

# choose all but crim, age, medv
house_prices_tbl %>% 
  select(-crim, -age, -medv)

# select columns between nox and tax
house_prices_tbl %>% 
  select(nox:tax)

# select all columns starting from "r"
house_prices_tbl %>% 
  select(starts_with("r"))

# select all columns containing "rat" 
house_prices_tbl %>% 
  select(contains("rat"))

# select all columns matching regular expression (ends with "s")
house_prices_tbl %>% 
  select(matches("s$"))

# select all columns matching regular expression (two "t"s with some letter(s) between)
house_prices_tbl %>% 
  select(matches("t.+t"))
```

Selecting columns id done by function `select()` with unquoted names of the columns you want to keep. You can also select columns by excluding some columns, with `-` before the name.

Sometimes is easier not to call names of ther columns directly. When we want to select columns which are next to each other we can use them using syntax `name_of_the_column_to_start:name_of_the_column_to_end`. We can also specify the names of columns we want to keep by calling `starts_with()` and `ends_with()` methods which will keep only the columns with the names starting (of ending) with string specified as an argument. Method `contains()` with some string as an argument will keep columns which contain in the name this string. Finally, we can also use regular expressions with the method `matches()` to select specific columns.

## Filter rows
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# filter rows
# rm - average number of rooms per dwelling > 8
house_prices_tbl %>% 
  filter(rm > 8)

# ptratio - pupil-teacher ratio < 15
# crim - per capita crime rate by town < 0.005
house_prices_tbl %>% 
  filter(ptratio < 15, crim < 0.02)
```

We can filter the rows by using function `filter()` with logical conditions on the columns as the arguments. The number of operations is limited but we can use comparison, mathematical and logical operators and many mathematical functions such as `log()` or `abs()`.

## Arrange rows
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# arrange by age in ascending order
house_prices_tbl %>% 
  arrange(age)

# arrange by crim in ascending order and by rm in descending order
house_prices_tbl %>% 
  arrange(crim, desc(rm))
```

To arrange rows in specific order we use `arrange()` function. Default sorting is in ascending order but we can use `desc()` method on the column to sort it in descending order.

## Change or add columns
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# change tax column by multiplying it by 10 000
house_prices_tbl %>% 
  mutate(tax = tax * 10000)

# create new column is_cheap which is transformation of medv column
house_prices_tbl %>% 
  mutate(is_cheap = ifelse(medv < 25, "cheap", "expensive"))

# create new column crim_medv_ratio which is crim to medv proportion
house_prices_tbl %>% 
  mutate(crim_medv_ratio = crim / medv)
```

Using `mutate()` function allows us to transform existing column or create a new one. The function takes arguments in the form `name_of_the_column = value_of_the_column`. When updating or adding the column you can operate on more than one column to count its value. 

## Calculate summary statistics
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# calculate mean medv
house_prices_tbl %>% 
  summarise(mean_medv = mean(medv))

# calculate min dis and sd of tax
house_prices_tbl %>% 
  summarise(min_dis = min(dis), sd_tax = sd(tax))
```
`summarise()` function is used to calculate summary of a column (e.g `min()`, `max()` or `mean()` value). The object returned by this function is also a tibble, even if it's only one single value.

## Select distinct rows
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# select ditinct values for column chas
house_prices_tbl %>% 
  distinct(chas)

# select distinct combinations of columns rad and chas
house_prices_tbl %>% 
  distinct(rad, chas)
```

Function `distinct()` is especially useful for categorical variables. When we call `distinct()` with only one column name as an argument we get unique values of this column. Two or more arguments will give us all unique combinations of values of the columns.

## Count unique combinations
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# count how many of each value from column chas we have
house_prices_tbl %>% 
  count(chas)

# count how many combinations of values from columns rad and chas we have
house_prices_tbl %>% 
  count(rad, chas, sort = TRUE)

# now take five the most common values of rad
house_prices_tbl %>% 
  count(rad, sort = TRUE) %>% 
  top_n(5)
```

If we want to count how many times the specific combinations of values occurs in our dataset we use function `count()` (with column(s) name(s) as an argument(s)). Additional argument is logical `sort` value which indicates if the result should be sorted in descending order by number of occurrences. Then we can chain this function with `top_n()` function to get top the most common combinations.

See that we can perform few operations one after the other by chaining it. It very useful and practical since usually we need more than one operation to get the desired result.

## Group by the same value
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# calculate average mean of medv in each group by rad
house_prices_tbl %>% 
  group_by(rad) %>% 
  summarise(medv_mean = mean(medv))

# mutate age by group-specific normalization (by chas)
house_prices_tbl %>% 
  group_by(chas) %>% 
  mutate(age_norm = (age - mean(age)) / sd(age)) %>% 
  select(chas, age, age_norm)
```

We use `group_by()` followed by `summarise()` or `mutate()` when we want to apply a summary statistic on each group or mutate column with group-specific values (respecively).

## Collect and compute data
```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
# assign results to new tibble
results <- house_prices_tbl %>% 
  filter(medv < 10) %>% 
  count(rad, chas)

# collect the results
collected <- results %>% collect()

# check class of two objects
class(results)
class(collected)

# compute the intermediate results and store in Spark
house_prices_tbl %>% 
  filter(medv < 10) %>%
  compute("house_prices_filtered")

# check if new object was created
src_tbls(conn)
```
We already know that tibble stores only a reference to the data, not the data itself. But if we want to move data back to R (for example to plot it) we can use `collect()` function. 

Copying data between Spark and R is slow, so it should only be performed when really needed. To store the result of intermediate calculations without collecting it we can use `compute()` function, which compute the calculations and stores the result in Spark. The function takes the string argument which is the name for new variable.

# Manipulating data using SQL syntax

The other way to manipulate data in Spark is to use raw SQL syntax. SQL queries are written as strings, and passed to `dbGetQuery()` from the `DBI` package. The are two differences when using SQL instead of `dplyr`:

- `dbGetQuery()` will always execute the query and return the results to R immediately,
- that `DBI` functions return data.frames rather than tibbles.

```{r, eval=FALSE, results=FALSE, warning=FALSE, message=FALSE}
library(DBI)

# take only nox, rm and age columns and return only 10 first rows
result <- dbGetQuery(conn, "SELECT nox, rm, age FROM house_prices LIMIT 10")

# check the class of result object
class(result)
```